# -*- coding: utf-8 -*-
"""ANN_BinaryVLE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11VGkk5xnbH5B4jAj7-f152sz1sNcGnxE
"""

# FOSSEE VLE ANN Modeling Screening Task
# ROBUST VERSION with Regularization and Weighted Loss
# ==============================================================================
# SECTION 1: SETUP AND THERMODYNAMIC CONSTANTS
# ==============================================================================
import numpy as np
import pandas as pd
from scipy.optimize import fsolve
import tensorflow as tf
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import time
import os
from itertools import product

print("Starting the VLE ANN Modeling script (Robust Version)...")

# Constants for Ethanol (1) - Water (2) system
P_SYSTEM_KPA = 101.325
# Antoine Coefficients
A1, B1, C1 = 8.1122, 1592.864, 226.184
A2, B2, C2 = 7.96681, 1668.21, 228.0
# Wilson Model Parameters
V1, V2 = 58.68, 18.07
LAMBDA_12, LAMBDA_21 = 0.1296, 0.4779


# ==============================================================================
# SECTION 2: THERMODYNAMIC DATA GENERATION
# ==============================================================================

def antoine_eq(T_k, A, B, C):
    """Calculates saturation pressure in kPa from temperature in Kelvin."""
    T_c = T_k - 273.15
    log_p_mmhg = A - (B / (T_c + C))
    p_mmhg = 10**log_p_mmhg
    return p_mmhg * 0.133322

def wilson_model(x1, lambda_12, lambda_21):
    """Calculates activity coefficients using the Wilson model."""
    x2 = 1 - x1
    G12 = np.exp(-lambda_12)
    G21 = np.exp(-lambda_21)
    ln_gamma1 = -np.log(x1 + x2 * G12) + x2 * (G12 / (x1 + x2 * G12) - G21 / (x2 + x1 * G21))
    ln_gamma2 = -np.log(x2 + x1 * G21) - x1 * (G12 / (x1 + x2 * G12) - G21 / (x2 + x1 * G21))
    return np.exp(ln_gamma1), np.exp(ln_gamma2)

def generate_vle_data(num_points=500):
    """Generates a thermodynamically consistent VLE dataset."""
    print(f"\nGenerating {num_points} VLE data points...")
    start_time = time.time()
    x1_range_1 = np.linspace(0.001, 0.7, int(num_points*0.4))
    x1_range_2 = np.linspace(0.701, 0.95, int(num_points*0.5))
    x1_range_3 = np.linspace(0.951, 0.999, int(num_points*0.1))
    x1_values = np.concatenate([x1_range_1, x1_range_2, x1_range_3])
    data = []
    for x1 in x1_values:
        def bubble_point_residual(T_k):
            gamma1, gamma2 = wilson_model(x1, LAMBDA_12, LAMBDA_21)
            p1_sat = antoine_eq(T_k, A1, B1, C1)
            p2_sat = antoine_eq(T_k, A2, B2, C2)
            p_calc = x1 * gamma1 * p1_sat + (1 - x1) * gamma2 * p2_sat
            return p_calc - P_SYSTEM_KPA
        T_initial_guess_k = 360.0
        T_eq_k = fsolve(bubble_point_residual, T_initial_guess_k)[0]
        gamma1, _ = wilson_model(x1, LAMBDA_12, LAMBDA_21)
        p1_sat = antoine_eq(T_eq_k, A1, B1, C1)
        y1 = (x1 * gamma1 * p1_sat) / P_SYSTEM_KPA
        data.append({'x1': x1, 'T': T_eq_k, 'P': P_SYSTEM_KPA, 'gamma1': gamma1, 'y1': y1})

    df = pd.DataFrame(data)

    output_folder = 'generated_dataset'
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    file_path = os.path.join(output_folder, 'vle_data.csv')
    df.to_csv(file_path, index=False)

    end_time = time.time()
    print(f"Data generation complete. Saved to '{file_path}'. Time taken: {end_time - start_time:.2f} seconds.")
    return df

# Generate data
df = generate_vle_data()


# ==============================================================================
# SECTION 3: DATA PREPROCESSING AND TRAIN-TEST SPLIT
# ==============================================================================
print("\nPreprocessing data and splitting into training and test sets...")
X_df = df[['x1', 'T', 'P', 'gamma1']]
y_s = df['y1']

X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(
    X_df, y_s, test_size=0.2, random_state=42
)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_df)
X_test_scaled = scaler.transform(X_test_df)

y_train = y_train_s.values
y_test = y_test_s.values

# --- MODIFICATION: Create Sample Weights ---
print("Creating sample weights to focus on the azeotrope region...")
# Increased weight to make the azeotrope region even more important
azeotrope_weight = 20.0
azeotrope_x1_min = 0.85
azeotrope_x1_max = 0.95

sample_weights = np.ones(len(y_train))
azeotrope_indices = (X_train_df['x1'] >= azeotrope_x1_min) & (X_train_df['x1'] <= azeotrope_x1_max)
sample_weights[azeotrope_indices] = azeotrope_weight
print(f"Applied a weight of {azeotrope_weight} to {np.sum(azeotrope_indices)} samples in the training set.")


print(f"\nData split into: {len(X_train_scaled)} training samples and {len(X_test_scaled)} test samples.")


# ==============================================================================
# SECTION 4: HYPERPARAMETER TUNING ON TRAINING DATA
# ==============================================================================
print("\nPerforming hyperparameter tuning using K-Fold CV on the TRAINING set...")
layer_sizes = [(32, 32), (64, 64), (128, 64)]
learning_rates = [0.001, 0.005]
batch_sizes = [16, 32]

best_val_rmse = float('inf')
best_params = None
best_history_log = None

kf = KFold(n_splits=5, shuffle=True, random_state=42)

for layers, lr, batch_size in product(layer_sizes, learning_rates, batch_sizes):
    print(f"\nTesting: Layers={layers}, LR={lr}, Batch={batch_size}")
    fold_val_rmses = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):
        X_t, X_v = X_train_scaled[train_idx], X_train_scaled[val_idx]
        y_t, y_v = y_train[train_idx], y_train[val_idx]
        weights_t = sample_weights[train_idx]

        # MODIFICATION: Add Kernel Regularization to prevent overfitting
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(4,)),
            tf.keras.layers.Dense(layers[0], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(layers[1], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
        model.compile(optimizer=optimizer, loss='mean_squared_error')

        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=15, restore_best_weights=True
        )

        history = model.fit(
            X_t, y_t, validation_data=(X_v, y_v),
            sample_weight=weights_t,
            epochs=200, batch_size=batch_size,
            callbacks=[early_stopping], verbose=0
        )

        y_pred_val = model.predict(X_v, verbose=0).flatten()
        val_rmse = np.sqrt(mean_squared_error(y_v, y_pred_val))
        fold_val_rmses.append(val_rmse)

    mean_val_rmse = np.mean(fold_val_rmses)
    print(f"  -> Mean Validation RMSE: {mean_val_rmse:.5f}")

    if mean_val_rmse < best_val_rmse:
        best_val_rmse = mean_val_rmse
        best_params = {'layers': layers, 'lr': lr, 'batch_size': batch_size}
        best_history_log = history.history

print("\n---------------------------------------------------------")
print(f"Best Hyperparameters Found:")
print(f"  - Layers: {best_params['layers']}, LR: {best_params['lr']}, Batch: {best_params['batch_size']}")
print(f"  - Best Mean Validation RMSE: {best_val_rmse:.5f}")
print("---------------------------------------------------------")


# ==============================================================================
# SECTION 5: FINAL MODEL TRAINING & EVALUATION ON TEST SET
# ==============================================================================
print("\nTraining final model on the full training set with best hyperparameters...")
final_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(4,)),
    tf.keras.layers.Dense(best_params['layers'][0], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dense(best_params['layers'][1], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['lr'])
final_model.compile(optimizer=final_optimizer, loss='mean_squared_error')
start_time = time.time()
final_model.fit(
    X_train_scaled, y_train,
    sample_weight=sample_weights,
    epochs=250,
    batch_size=best_params['batch_size'],
    verbose=0
)
end_time = time.time()
print(f"Final model training complete. Time taken: {end_time - start_time:.2f} seconds.")

print("\n--- FINAL EVALUATION ON UNSEEN TEST SET ---")
y_pred_ann = final_model.predict(X_test_scaled, verbose=0).flatten()
rmse_ann = np.sqrt(mean_squared_error(y_test, y_pred_ann))
mae_ann = mean_absolute_error(y_test, y_pred_ann)
print(f"ANN Performance on Test Set:")
print(f"  - Root Mean Squared Error (RMSE): {rmse_ann:.5f}")
print(f"  - Mean Absolute Error (MAE):   {mae_ann:.5f}")

# Baseline: Raoult's Law on the test set
y_pred_raoult = []
for index, row in X_test_df.iterrows():
    p1_sat = antoine_eq(row['T'], A1, B1, C1)
    y_r = (row['x1'] * p1_sat) / row['P']
    y_pred_raoult.append(y_r)
rmse_raoult = np.sqrt(mean_squared_error(y_test, y_pred_raoult))
mae_raoult = mean_absolute_error(y_test, y_pred_raoult)
print(f"\nRaoult's Law (Baseline) Performance on Test Set:")
print(f"  - Root Mean Squared Error (RMSE): {rmse_raoult:.5f}")
print(f"  - Mean Absolute Error (MAE):   {mae_raoult:.5f}")

# Azeotrope Detection
azeotrope_idx = np.argmin(np.abs(y_pred_ann - X_test_df['x1'].values))
azeotrope_x1 = X_test_df['x1'].iloc[azeotrope_idx]
azeotrope_y1 = y_pred_ann[azeotrope_idx]
azeotrope_T_K = X_test_df['T'].iloc[azeotrope_idx]
reference_x1 = 0.894
reference_T_K = 351.3
print(f"\nAzeotrope Analysis:")
print(f"  - Predicted Azeotrope: x1 = {azeotrope_x1:.4f}, y1 = {azeotrope_y1:.4f}, T = {azeotrope_T_K:.2f} K")
print(f"  - Reference Azeotrope: x1 = {reference_x1:.4f}, T = {reference_T_K:.2f} K")
print(f"  - Absolute Error in x1: {abs(azeotrope_x1 - reference_x1):.4f}")
print(f"  - Absolute Error in T: {abs(azeotrope_T_K - reference_T_K):.2f} K")


# ==============================================================================
# SECTION 6: VISUALIZATION
# ==============================================================================
print("\nGenerating final plots...")
plt.figure(figsize=(16, 12))

# Plot 1: Training Loss of Best CV Fold
plt.subplot(2, 2, 1)
plt.plot(best_history_log['loss'], label='Training Loss')
plt.plot(best_history_log['val_loss'], label='Validation Loss')
plt.title('Best Fold: Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)
plt.ylim(bottom=0)

# Plot 2: Parity Plot on Test Data
plt.subplot(2, 2, 2)
plt.scatter(y_test, y_pred_ann, alpha=0.6, label='ANN Predictions (Test Set)')
plt.plot([0, 1], [0, 1], 'r--', label='Ideal Line (y=x)')
plt.title('Parity Plot on Unseen Test Data')
plt.xlabel('Actual y1')
plt.ylabel('Predicted y1')
plt.legend()
plt.grid(True)
plt.axis('square')

# Plot 3: y-x Diagram on Test Data
plt.subplot(2, 2, 3)
sorted_indices = np.argsort(X_test_df['x1'])
x1_sorted = X_test_df['x1'].iloc[sorted_indices]
y1_pred_sorted = y_pred_ann[sorted_indices]
plt.plot(x1_sorted, y1_pred_sorted, 'b-', label='ANN Predicted y1 (Test Set)', linewidth=2)
plt.scatter(X_test_df['x1'], y_test, color='orange', s=10, alpha=0.5, label='Actual Test Data')
plt.plot([0, 1], [0, 1], 'r--', label='y1 = x1')
plt.plot(azeotrope_x1, azeotrope_y1, 'go', markersize=10, label=f'Predicted Azeotrope\nx1={azeotrope_x1:.3f}, T={azeotrope_T_K:.1f}K')
plt.title('y-x Diagram on Unseen Test Data')
plt.xlabel('Liquid Mole Fraction, x1 (Ethanol)')
plt.ylabel('Vapor Mole Fraction, y1 (Ethanol)')
plt.legend()
plt.grid(True)
plt.xlim(0, 1)
plt.ylim(0, 1)

# Plot 4: Residual Plot on Test Data
plt.subplot(2, 2, 4)
residuals = y_test - y_pred_ann
plt.scatter(X_test_df['x1'], residuals, alpha=0.6, color='purple')
plt.axhline(0, color='black', linestyle='--')
plt.title('Residuals on Unseen Test Data')
plt.xlabel('Liquid Mole Fraction, x1 (Ethanol)')
plt.ylabel('Residual (y_actual - y_predicted)')
plt.grid(True)

plt.tight_layout()
plt.suptitle('Final Regularized & Weighted Loss ANN Model Analysis', fontsize=16, y=1.02)
plt.show()

print("\nScript finished successfully.")